
@inproceedings{hu2020heterogeneous,
  title={Heterogeneous graph transformer},
  author={Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Sun, Yizhou},
  booktitle={Proceedings of the web conference 2020},
  pages={2704--2710},
  year={2020}
}

@article{jarvelin_cumulated_2002,
	title = {Cumulated gain-based evaluation of {IR} techniques},
	volume = {20},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/582415.582418},
	doi = {10.1145/582415.582418},
	abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
	number = {4},
	urldate = {2023-02-09},
	journal = {ACM Transactions on Information Systems},
	author = {Järvelin, Kalervo and Kekäläinen, Jaana},
	month = oct,
	year = {2002},
	keywords = {cumulated gain, Graded relevance judgments},
	pages = {422--446},
}


@article{sun_pathsim_2020,
	title = {{PathSim}: meta path-based top-{K} similarity search in heterogeneous information networks},
	volume = {4},
	issn = {2150-8097},
	shorttitle = {{PathSim}},
	url = {https://doi.org/10.14778/3402707.3402736},
	doi = {10.14778/3402707.3402736},
	abstract = {Similarity search is a primitive operation in database and Web search engines. With the advent of large-scale heterogeneous information networks that consist of multi-typed, interconnected objects, such as the bibliographic networks and social media networks, it is important to study similarity search in such networks. Intuitively, two objects are similar if they are linked by many paths in the network. However, most existing similarity measures are defined for homogeneous networks. Different semantic meanings behind paths are not taken into consideration. Thus they cannot be directly applied to heterogeneous networks. In this paper, we study similarity search that is defined among the same type of objects in heterogeneous networks. Moreover, by considering different linkage paths in a network, one could derive various similarity semantics. Therefore, we introduce the concept of meta path-based similarity, where a meta path is a path consisting of a sequence of relations defined between different object types (i.e., structural paths at the meta level). No matter whether a user would like to explicitly specify a path combination given sufficient domain knowledge, or choose the best path by experimental trials, or simply provide training examples to learn it, meta path forms a common base for a network-based similarity search engine. In particular, under the meta path framework we define a novel similarity measure called PathSim that is able to find peer objects in the network (e.g., find authors in the similar field and with similar reputation), which turns out to be more meaningful in many scenarios compared with random-walk based similarity measures. In order to support fast online query processing for PathSim queries, we develop an efficient solution that partially materializes short meta paths and then concatenates them online to compute top-k results. Experiments on real data sets demonstrate the effectiveness and efficiency of our proposed paradigm.},
	number = {11},
	urldate = {2023-01-12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Sun, Yizhou and Han, Jiawei and Yan, Xifeng and Yu, Philip S. and Wu, Tianyi},
	month = jun,
	year = {2020},
	pages = {992--1003},
}


@inproceedings{amba_dynamic_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {Dynamic {Language} {Models} for {Continuously} {Evolving} {Content}},
	isbn = {978-1-4503-8332-5},
	url = {https://doi.org/10.1145/3447548.3467162},
	doi = {10.1145/3447548.3467162},
	abstract = {The content on the web is in a constant state of flux. New entities,issues, and ideas continuously emerge, while the semantics of the existing conversation topics gradually shift. In recent years, pre-trained language models like BERT greatly improved the state-of-the-art for a large spectrum of content understanding tasks.Therefore, in this paper, we aim to study how these language models can be adapted to better handle continuously evolving web content.In our study, we first analyze the evolution of 2013 - 2019 Twitter data, and unequivocally confirm that a BERT model trained on past tweets would heavily deteriorate when directly applied to data from later years. Then, we investigate two possible sources of the deterioration: the semantic shift of existing tokens and the sub-optimal or failed understanding of new tokens. To this end, we both explore two different vocabulary composition methods, as well as propose three sampling methods which help in efficient incremental training for BERT-like models. Compared to a new model trained from scratch offline, our incremental training (a) reduces the training costs, (b) achieves better performance on evolving content, and (c)is suitable for online deployment. The superiority of our methods is validated using two downstream tasks. We demonstrate significant improvements when incrementally evolving the model from a particular base year, on the task of Country Hashtag Prediction, as well as on the OffensEval 2019 task.},
	urldate = {2023-01-12},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Amba Hombaiah, Spurthi and Chen, Tao and Zhang, Mingyang and Bendersky, Michael and Najork, Marc},
	month = aug,
	year = {2021},
	keywords = {active learning, dynamic vocabulary, hard example mining, incremental learning, language modeling, vocabulary composition},
	pages = {2514--2524},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-01-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/user/Zotero/storage/ZP5L5XVK/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{garcia_few_shot_2017,
  doi = {10.48550/ARXIV.1711.04043},
  
  url = {https://arxiv.org/abs/1711.04043},
  
  author = {Garcia, Victor and Bruna, Joan},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Few-Shot Learning with Graph Neural Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{yin_gsdmm_2014,
	location = {New York, {NY}, {USA}},
	title = {A dirichlet multinomial mixture model-based approach for short text clustering},
	isbn = {978-1-4503-2956-9},
	url = {https://doi.org/10.1145/2623330.2623715},
	doi = {10.1145/2623330.2623715},
	series = {{KDD} '14},
	abstract = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to {GSDMM}). We found that {GSDMM} can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. {GSDMM} can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that {GSDMM} can achieve significantly better performance than three other clustering models.},
	pages = {233--242},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Yin, Jianhua and Wang, Jianyong},
	urldate = {2023-03-23},
	date = {2014-08-24},
	keywords = {dirichlet multinomial mixture, gibbs sampling, short text clustering},
	file = {Yin and Wang - 2014 - A dirichlet multinomial mixture model-based approa.pdf:/home/user/Zotero/storage/8UXSP37G/Yin and Wang - 2014 - A dirichlet multinomial mixture model-based approa.pdf:application/pdf},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {{arXiv}:1312.6114},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2023-03-14},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ward_hierarchical_1963,
	title = {Hierarchical Grouping to Optimize an Objective Function},
	volume = {58},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
	doi = {10.1080/01621459.1963.10500845},
	abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale (n {\textgreater} 100) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n − 1 mutually exclusive sets by considering the union of all possible n(n − 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
	pages = {236--244},
	number = {301},
	journaltitle = {Journal of the American Statistical Association},
	author = {Ward, Joe H.},
	urldate = {2023-03-26},
	date = {1963-03-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1963.10500845},
}

@article{zhao_heterogeneous_2021,
	title = {Heterogeneous Graph Structure Learning for Graph Neural Networks},
	volume = {35},
	rights = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16600},
	doi = {10.1609/aaai.v35i5.16600},
	abstract = {Heterogeneous Graph Neural Networks ({HGNNs}) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing {HGNNs} relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for {HGNNs} rather than rely only on the raw graph structure. In light of this, we make the first attempt towards learning an optimal heterogeneous graph structure for {HGNNs} and propose a novel framework {HGSL}, which jointly performs Heterogeneous Graph Structure Learning and {GNN} parameters learning for classification task. Different from traditional {GSL} on homogeneous graph, considering the heterogeneity of different relations in heterogeneous graph, {HGSL} generates each relation subgraph independently. Specifically, in each generated relation subgraph, {HGSL} not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a {GNN} towards classification objective. Extensive experiments on real-world graphs demonstrate that the proposed framework significantly outperforms the state-of-the-art methods.},
	pages = {4697--4705},
	number = {5},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Zhao, Jianan and Wang, Xiao and Shi, Chuan and Hu, Binbin and Song, Guojie and Ye, Yanfang},
	urldate = {2023-02-24},
	date = {2021-05-18},
	langid = {english},
	note = {Number: 5},
	keywords = {Semi-Supervised Learning},
}

@article{stanley_compressing_2018,
	title = {Compressing Networks with Super Nodes},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-29174-3},
	doi = {10.1038/s41598-018-29174-3},
	abstract = {Community detection is a commonly used technique for identifying groups in a network based on similarities in connectivity patterns. To facilitate community detection in large networks, we recast the network as a smaller network of ‘super nodes’, where each super node comprises one or more nodes of the original network. We can then use this super node representation as the input into standard community detection algorithms. To define the seeds, or centers, of our super nodes, we apply the ‘{CoreHD}’ ranking, a technique applied in network dismantling and decycling problems. We test our approach through the analysis of two common methods for community detection: modularity maximization with the Louvain algorithm and maximum likelihood optimization for fitting a stochastic block model. Our results highlight that applying community detection to the compressed network of super nodes is significantly faster while successfully producing partitions that are more aligned with the local network connectivity and more stable across multiple (stochastic) runs within and between community detection algorithms, yet still overlap well with the results obtained using the full network.},
	pages = {10892},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Stanley, Natalie and Kwitt, Roland and Niethammer, Marc and Mucha, Peter J.},
	urldate = {2023-03-14},
	date = {2018-07-18},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Statistics},
}

@article{fortunato_community_2010,
	title = {Community detection in graphs},
	volume = {486},
	issn = {0370-1573},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	doi = {10.1016/j.physrep.2009.11.002},
	abstract = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.},
	pages = {75--174},
	number = {3},
	journaltitle = {Physics Reports},
	shortjournal = {Physics Reports},
	author = {Fortunato, Santo},
	urldate = {2023-03-28},
	date = {2010-02-01},
	langid = {english},
	keywords = {Clusters, Graphs, Statistical physics},
}

@misc{besta_survey_2019,
	title = {Survey and Taxonomy of Lossless Graph Compression and Space-Efficient Graph Representations},
	url = {http://arxiv.org/abs/1806.01799},
	doi = {10.48550/arXiv.1806.01799},
	abstract = {Various graphs such as web or social networks may contain up to trillions of edges. Compressing such datasets can accelerate graph processing by reducing the amount of I/O accesses and the pressure on the memory subsystem. Yet, selecting a proper compression method is challenging as there exist a plethora of techniques, algorithms, domains, and approaches in compressing graphs. To facilitate this, we present a survey and taxonomy on lossless graph compression that is the first, to the best of our knowledge, to exhaustively analyze this domain. Moreover, our survey does not only categorize existing schemes, but also explains key ideas, discusses formal underpinning in selected works, and describes the space of the existing compression schemes using three dimensions: areas of research (e.g., compressing web graphs), techniques (e.g., gap encoding), and features (e.g., whether or not a given scheme targets dynamic graphs). Our survey can be used as a guide to select the best lossless compression scheme in a given setting.},
	number = {{arXiv}:1806.01799},
	publisher = {{arXiv}},
	author = {Besta, Maciej and Hoefler, Torsten},
	urldate = {2023-04-13},
	date = {2019-04-27},
	eprinttype = {arxiv},
	eprint = {1806.01799 [cs, math]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, Computer Science - Information Theory},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1126--1135},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	urldate = {2023-03-14},
	date = {2017-07-17},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{wang_embeddings_2022,
  author={Wang, Xiao and Bo, Deyu and Shi, Chuan and Fan, Shaohua and Ye, Yanfang and Yu, Philip S.},
  journal={IEEE Transactions on Big Data}, 
  title={A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TBDATA.2022.3177455}
 }